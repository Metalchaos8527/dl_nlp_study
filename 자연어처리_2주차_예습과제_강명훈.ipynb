{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망의 학습구현\n",
    "\n",
    "##  손실함수 Loss Function\n",
    "\n",
    "- 손실을 산출하기 위한 수식들을 구현한 함수\n",
    "- 손실 Loss는 신경망이 예측한 결과와 실제 정답사이의 오차를 나타내는 단일 값(스칼라)\n",
    "- 손실을 구하기 위해서는 먼저 은닉층을 거쳐 변환된 입력값들을 이용하여 softmax함수를 적용하여 새로운 값으로 반환한다\n",
    "- 이 때의 값은 어떤 클래스들에 대한 소속 확률을 말한다\n",
    "\n",
    "## Softmax 함수\n",
    "https://wikidocs.net/35476\n",
    "## $y_k = \\frac{exp(s_k)}{\\sum_{i = 1}^{n}exp(s_i)}$\n",
    "- 출력이 n개일 때, k번째 출력 $y_k$, 분자는 K번째 클래스의 점수\n",
    "- 분모는 모든 클래스들의 점수\n",
    "- 따라서 결국 모든 클래스들의 소프트맥스 함수값을 더하면 1이 산출됨\n",
    "- 때문에 소프트맥스 출력값을 특정 클래스에 속할 확률로 보는 것\n",
    "\n",
    "## Cross Entropy 함수\n",
    "https://wikidocs.net/35476\n",
    "## $L = -\\sum_{k}t_k\\log y_k$\n",
    "- 여기서 로그는 자연로그를 의미\n",
    "- $t_k$는 k번째 클래스에 해당하는 정답 레이블 \n",
    "- $t_k$가 **원-핫벡터**이므로 정답에 해당하는 클래스만의 오차를 구하게 됨\n",
    "- 만약 $y_k$가 1이면 $log1 = 0$ 이므로 손실함수 값도 0\n",
    "- 따라서 Cross Entropy함수의 값이 최소화되기 위해서는 $y_k = 1$이 되는, 즉 정확한 예측을 해야하므로\n",
    "- 손실함수로써 적합함\n",
    "\n",
    "## 미니배치 작동방식의 Cross Entropy 함수\n",
    "## $L = -\\frac{1}{N}\\sum_{n}\\sum_{k}t_{nk}\\log y_{nk}$\n",
    "- 각 데이터, 케이스에 대해서 손실을 계산해야 하므로\n",
    "- 데이터의 개수 n과 $\\sum_{n}$이 추가 되고\n",
    "- 평균 손실함수를 구하게 되므로 $\\frac{1}{N}$이 추가됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax 함수 구현\n",
    "# https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/common/functions.py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim >= 2: #입력이 2차원 이상일 때\n",
    "        x = x - x.max(axis = 1, keepdims = True) #오버플로우를 막기 위해\n",
    "        #예로 1000이라는 값이 들어가면 exp(1000)이 계산되지 못함\n",
    "        #따라서 수들을 가장 큰 수를 바탕으로 각각 빼주고 계산을 수행\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis = 1, keepdims = True) #여기서 x는 np.exp(x)를 말함\n",
    "        \n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x) #1차원이기 때문에 axis지정 필요가 없음\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "                \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Entropy 함수 구현\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    #y는 소프트맥스 반환 값, t는 실제 정답데이터\n",
    "    \n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis = 1) #정답 1인 인덱스 값을 반환\n",
    "        #즉 만약 3개의 클래스가 있다면 나올 수 있는 인덱스는 0, 1, 2\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "    # t_k * log y_k는 결국 y의 정답에 해당하는 클래스의 로그값만 산출하면 되기 떄문에\n",
    "    # np.log안에서 y의 인덱싱을 사용하는 것\n",
    "    # 행은 배치사이즈의 개수, 즉 케이스의 개수들을 사용 해야하기 때문에 \n",
    "    # range와 같은 기능을 하는 np.arange를 사용\n",
    "    # 열은 정답에 해당하는 클래스를 알아야 하므로 t.argmax를 사용하여 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 미분과 기울기\n",
    "- 신경망 학습의 목표는 손실을 최소화하는 매개변수params를 찾는것\n",
    "- x에 대한 y의 미분값은 x의 변화에 따른 y의 변화량을 의미하며\n",
    "- 이때의 변화량은 기울기를 의미\n",
    "- 왜 기울기를 구하는가?  \n",
    "> 우리가 기울기를 알게 되면 어떤 변수의 변화에 따라서 y가 변화하는 정도를 알 수 있게 되기 때문  \n",
    "> 신경망에서의 기울기는 손실에 대한 기울기      \n",
    "> 즉 각 가중치가 손실을 발생시키는것에 얼마나 영향을 미치는지를 말하는것     \n",
    "> **기울기를 조정**하여 y의 변화의 정도를 달리할 때 결과를 비교하여서   \n",
    "> 보다 정확한 y의 결과를 산출하도록 조정할 수 있을 것 -> 결국 기울기를 변동시키는일\n",
    "\n",
    "## 신경망에서의 역전파\n",
    "- 신경망 학습의 목적은 **손실을 최소화** 하는 것\n",
    "- 손실을 최소화 하기 위해서는 매번 계산되는 오차를 줄이는 방향으로 학습이 되어야 함\n",
    "- 신경망에서의 학습의 주요 파라미터는 가중치\n",
    "- 즉 가중치들을 업데이트 해야 손실도 업데이트\n",
    "- 그런데 가중치들마다 **손실에 영향을 주는 정도가 다를 것**\n",
    "- 손실에 영향을 주는 정도는 기울기\n",
    "- 신경망은 여러 함수들의 합성으로 이루어지는 거대 **합성함수**\n",
    "- 따라서 기울기는 합성함수의 미분, 즉 연쇄법칙을 이용해서 기울기를 구할 수 있다\n",
    "- 이렇게 연쇄법칙을 사용하면 최종 출력층에서 최초 입력층까지 가는 층들 사이의 모든 기울기들을 구할 수 있게 된다\n",
    "- 이렇게 구해진 **기울기들에 대한 튜닝**이 이루어지는게 신경망에서의 학습이다\n",
    "\n",
    "## 신경망 구현을 위한 다양한 노드들\n",
    "- 덧셈노드: 어떤 계산된 값들을 더하는 역할 수행\n",
    "- 곱셈노드: 어떤 계산된 값들을 곱하는 역할 수행\n",
    "- 분기노드: 어떤 값들을 다음의 노드들로 복제하여 이동시키는 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat 노드 구현, 분기노드의 일종\n",
    "# 즉 N개로 분기하는 노드가 Repeat 노드\n",
    "\n",
    "D, N = 8, 7 #D는 차원 수, N은 케이스 개수\n",
    "\n",
    "#순전파\n",
    "#여기서 x는 가중치라고 가정함\n",
    "x = np.random.randn(1, D) #반복하고자 하는 입력 x\n",
    "y = np.repeat(x, N, axis = 0) #x를 케이스 개수 N만큼 반복하여 생성\n",
    "    #행단위로 생성되기 때문에 axis = 0\n",
    "\n",
    "#역전파\n",
    "dy = np.random.randn(N, D)\n",
    "dx = np.sum(dy, axis = 0, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.35832679,  1.42660931,  0.42843892, -0.62493969,  0.97154252,\n",
       "         0.06677258,  0.22583876, -0.0750021 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.35832679,  1.42660931,  0.42843892, -0.62493969,  0.97154252,\n",
       "         0.06677258,  0.22583876, -0.0750021 ],\n",
       "       [ 0.35832679,  1.42660931,  0.42843892, -0.62493969,  0.97154252,\n",
       "         0.06677258,  0.22583876, -0.0750021 ],\n",
       "       [ 0.35832679,  1.42660931,  0.42843892, -0.62493969,  0.97154252,\n",
       "         0.06677258,  0.22583876, -0.0750021 ],\n",
       "       [ 0.35832679,  1.42660931,  0.42843892, -0.62493969,  0.97154252,\n",
       "         0.06677258,  0.22583876, -0.0750021 ],\n",
       "       [ 0.35832679,  1.42660931,  0.42843892, -0.62493969,  0.97154252,\n",
       "         0.06677258,  0.22583876, -0.0750021 ],\n",
       "       [ 0.35832679,  1.42660931,  0.42843892, -0.62493969,  0.97154252,\n",
       "         0.06677258,  0.22583876, -0.0750021 ],\n",
       "       [ 0.35832679,  1.42660931,  0.42843892, -0.62493969,  0.97154252,\n",
       "         0.06677258,  0.22583876, -0.0750021 ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.58705079,  0.86673324, -0.05114883,  0.83979214, -0.0545844 ,\n",
       "         2.25655951, -2.14715897,  0.98560426],\n",
       "       [-0.4927982 ,  1.52041027,  0.09259118, -1.5413771 , -0.28601281,\n",
       "         0.6477942 ,  0.11908493,  0.93688839],\n",
       "       [-0.28496575, -0.8571765 ,  0.45112044, -0.37726422, -0.664151  ,\n",
       "         0.5211662 ,  1.18862862,  1.00632612],\n",
       "       [-2.08689169,  0.16120828,  2.15293996,  0.79785055,  1.74885718,\n",
       "        -2.26928078, -1.07280458,  1.12038807],\n",
       "       [-1.71690516, -0.66746499,  0.65167683,  0.97956274,  1.42153392,\n",
       "        -0.91776403, -0.11173555,  0.08144114],\n",
       "       [ 0.84242643, -0.63090211,  0.16342902, -0.32291178,  0.01346453,\n",
       "        -1.08847955,  0.43452648, -0.81062004],\n",
       "       [-0.01574223, -0.80162899, -0.40268858,  0.62488733,  1.91477952,\n",
       "        -0.71480347, -0.65221657,  1.68182359]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.3419274 , -0.40882081,  3.05792003,  1.00053965,  4.09388694,\n",
       "        -1.56480791, -2.24167564,  5.00185153]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum 노드\n",
    "#Repeat노드의 반대\n",
    "\n",
    "#순전파\n",
    "D, N = 8, 7\n",
    "\n",
    "x = np.random.randn(N, D)\n",
    "y = np.sum(x, axis = 0, keepdims = True)\n",
    "\n",
    "#역전파\n",
    "dy = np.random.randn(1, D)\n",
    "dx = np.repeat(dy, N, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.65290865, -0.11929516,  0.85045447, -0.746642  , -0.22061344,\n",
       "        -0.06053845, -0.50850599, -1.26458318],\n",
       "       [ 0.92851761, -1.1535945 ,  1.47765004, -1.03468855, -0.54629744,\n",
       "        -1.10727647,  0.48388698, -0.07466376],\n",
       "       [ 0.30421126, -0.05080099,  1.44816276,  0.79658339, -0.84989139,\n",
       "         0.62706366,  0.32516328,  0.4687517 ],\n",
       "       [ 0.46598034,  1.66151729, -2.7459132 , -0.83415049, -0.37079479,\n",
       "         0.70181227, -2.1096123 , -0.09015812],\n",
       "       [-0.63646922, -0.32474407, -0.91905948, -0.45409248,  0.66499036,\n",
       "        -1.41825484,  0.58028088, -2.06765433],\n",
       "       [ 0.13619767, -0.30516651,  0.03760699,  2.69256849, -0.15980494,\n",
       "        -0.4350568 ,  1.96331884, -0.39232803],\n",
       "       [ 0.51254539, -0.23737964,  0.17545838,  0.6036519 ,  0.0518461 ,\n",
       "        -0.02517837,  0.25615402, -1.21947134]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.36389169, -0.52946358,  0.32435996,  1.02323025, -1.43056555,\n",
       "        -1.717429  ,  0.99068571, -4.64010708]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.86903658, -0.32281983,  1.23746052,  0.43589973,  0.60242731,\n",
       "         0.04704064,  1.78499222, -0.46355715]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.86903658, -0.32281983,  1.23746052,  0.43589973,  0.60242731,\n",
       "         0.04704064,  1.78499222, -0.46355715],\n",
       "       [ 0.86903658, -0.32281983,  1.23746052,  0.43589973,  0.60242731,\n",
       "         0.04704064,  1.78499222, -0.46355715],\n",
       "       [ 0.86903658, -0.32281983,  1.23746052,  0.43589973,  0.60242731,\n",
       "         0.04704064,  1.78499222, -0.46355715],\n",
       "       [ 0.86903658, -0.32281983,  1.23746052,  0.43589973,  0.60242731,\n",
       "         0.04704064,  1.78499222, -0.46355715],\n",
       "       [ 0.86903658, -0.32281983,  1.23746052,  0.43589973,  0.60242731,\n",
       "         0.04704064,  1.78499222, -0.46355715],\n",
       "       [ 0.86903658, -0.32281983,  1.23746052,  0.43589973,  0.60242731,\n",
       "         0.04704064,  1.78499222, -0.46355715],\n",
       "       [ 0.86903658, -0.32281983,  1.23746052,  0.43589973,  0.60242731,\n",
       "         0.04704064,  1.78499222, -0.46355715]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matmul 노드, 곱셈 노드 구현\n",
    "#입력 X와 가중치 W를 곱하는 곱셈노드 \n",
    "import numpy as np\n",
    "\n",
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None #초기 입력은 없는 상태\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x #인스턴스 변수에 X를 할당\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T) #x의 역전파\n",
    "        dW = np.dot(self.x.T, dout) #W의 역전파\n",
    "        self.grads[0][...] = dW #여기서 [...]는 덮어쓰기 기능\n",
    "        #덮어쓰기는 처음 변수가 할당된 메모리 주소를 그대로 이용하는 것\n",
    "        #즉 처음 변수가 주어진 메모리에서 해당 변수의 데이터만 바꾸는 것\n",
    "        #따라서 변수 할당을 또 다른 메모리에 할 필요가 없으므로 가중치 업데이트에서 효율적임\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#시그모이드 계층 순전파, 역전파 구현\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#순전파 계층 구현\n",
    "#지금까지 구현한 Matmul, Sigmoid, Sum, Repeat을 사용\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W,b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)] #초기 가중치값 초기화\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b #여기서 b는 브로드캐스팅을 시행\n",
    "        #브로드캐스팅을 위해 b들이 repeat되므로 repeat노드 역할이 이뤄지는 것\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(x.T, dout)\n",
    "        db = np.sum(dout, axis = 0)\n",
    "        \n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#가중치 갱신\n",
    "#확률적 경사하강법 SGD 이용\n",
    "#확률적 경사하강법: 무작위로 선택된 데이터, 즉 미니배치에 대한 기울기를 이용하여 가중치 갱신\n",
    "#이 때 가중치 갱신은 \n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 확률적 경사하강법\n",
    "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=2&ab_channel=3Blue1Brown\n",
    "\n",
    "- 역전파로 계산된 가중치들은 다음을 나타낸다\n",
    "- 가중치의 부호는 해당 가중치들이 업데이트 될 때 커저야 하는지, 작아져야 하는지를 나타낸다\n",
    "- 만약 기울기가 양의 부호이면 해당 가중치가 작아져야 한다는 것이고, \n",
    "- 기울기가 음의 부호이면 해당 가중치가 커져야 한다는 것이다\n",
    "- 가중치의 절대값 크기는 해당 가중치들이 얼마나 중요하게 업데이트 되어야 하는지를 나타낸다\n",
    "- 만약 기울기의 절대값이 크다면 해당 가중치는 보다 많이 업데이트 되어야 하고\n",
    "- 기울기의 절대값이 작다면 해당 가중치는 보다 적게 업데이트 되어야 한다\n",
    "- 그리고 이 때 가중치 업데이트의 속도는 학습률, Learning Rate가 결정한다\n",
    "\n",
    "## DNN의 함정\n",
    "- Cost Function은 단순히 현재 주어진 데이터셋의 라벨값을 바탕으로 정답인지 아닌지를 가린다\n",
    "- 만약 기존의 라벨링이 없는 새로운 데이터가 DNN에 들어올 때 (혹은 라벨링이 우리가 가진 데이터셋 라벨의 범주를 벗어나는 경우)\n",
    "- DNN은 학습을 통해서 어떠한 결과를 도출해 낼 것이다\n",
    "- 그러나 해당 결과는 연구자의 기대와 다른 전혀 다른 결과이다 (우리는 애초에 input단계에서 데이터가 다른 형태임을 인지할 것이라고 기대하지만 그렇지 않다)\n",
    "- 그 이유는 Cost Function이 단순히 초기 랜덤하게 주어진 가중치들을 가지고 최소화만을 추구하는 기법을 다루기 때문이다\n",
    "- 기존의 우리가 가지고 있는 데이터셋은 정답과 오답을 명확하게 구분할 수 있어 Cost의 측정에 부합하고 이를 바탕으로 Cost Fucntion을 이용하여 최소화를 추구할 수 있다\n",
    "- 그러나 전혀 다른 형태의 데이터가 DNN에 투입되게 된다면 오답을 가릴 수 없으므로 Cost Funcnction이 제대로 작동하지 못한다\n",
    "- 따라서 기존의 데이터셋과 전혀 다른 데이터가 DNN에 투입될 경우 DNN은 제대로 성능을 낼 수 없으며\n",
    "- 설령 어떤 성능을 내더라도 그것이 제대로 학습을 했다고 볼 수 없다\n",
    "\n",
    "위에서 밝혀진 한계로 이후 다양한 신경망 기법들이 등장하게 되었다고 볼 수 있다\n",
    "- 보다 정확한 신경망이 되기 위해서는 Cost Function이 정교해져야하거나\n",
    "- 초기 input 단계에서 데이터를 거를 수 있는 방법이 개발되어야 할 것이다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
